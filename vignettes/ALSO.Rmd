---
title: "Attribute Wise Learning for Scoring Outliers"
author: "Danny Morris"
date: "July 18, 2018"
output: 
    html_document:
        theme: lumen
        toc: true
        df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      cache = FALSE)
```

# Overview

Attirubte-wise Learning for SCoring Outliers (ALSO) is an outlier detection technique for standard multidimensional datasets. For each variable in the dataset, a predictive model is constructed in which the remaining features are predictors. When a feature is generally predictable with good accuracy, observations deviating significantly from predicted values may be outliers. Observations with significant deviations across many predictable features may be strong outliers.

This document will illustrate the use of the **ALSO package**. It was created for the sole purpose of making the ALSO technique easily available to R users. The package currently contains a single function, `ALSO()`.

```{r}
library(ALSO) # devtools::install_github("dannymorris/ALSO")

library(dplyr)
library(corrplot)
library(plotly)

sessionInfo()
```

# Data

```{r}
states <- state.x77 %>% as_tibble()

states
```

This dataset is relatively small at 50 rows and 8 variables. Later on we'll explore a higher dimenional dataset and observe the results.

# Center and scale variables for preprocessing

It is customary to standardize numeric variables prior to predictive modeling in order to eliminate the effects of differences in original measurement scales. Let's apply the common z-standardization technique.

```{r}
states_scaled <- states %>%
    mutate_all(funs(scale))
```

# Univariate density plots

```{r}
par(mfrow = c(3,3),
    mar = c(3,3,2,2))

for (i in seq_along(states_scaled)) {
    pull(states_scaled[, i]) %>%
        density() %>%
        plot(main = colnames(states_scaled[, i]),
             xlab = "")
}
```

Population, Income, and Area show potential outliers in the right-side tails.

# Bivariate linear correlations

```{r}
pairs(states_scaled)

cor(states_scaled) %>% round(., 2)
```

The correlation plot and matrix show quite a few moderate to strong correlations in both positive and negative directions.

# ALSO with ordinary least squares regression

In spite of their simplicity, the ordinary least squares (OLS) regression model can be an overall effective solution. The current dataset is small, but OLS is a very efficient algorithm which is suitable for processing much larger datasets quickly. 

```{r}
lm_also <- ALSO(data = states_scaled, model_function = lm, cross_validate = F)
```

### Scores

```{r}

```

# ALSO with random forest regressor

A random forest works quite nicely in the context of ALSO for a few reasons:

1. can do prediction or classification
2. robust due to ensembling
3. can detect nonlinear relationships

```{r}
rf_also <- ALSO(data = states_scaled, model_function = randomForest::randomForest,
                cross_validate = FALSE, scores_only = F)
```

### Outlier scores

```{r}
tibble(state = rownames(state.x77), 
       outlier_score = rf_also$scores %>% as.vector) %>%
    arrange(desc(outlier_score))


plot(density(rf_also$scores), main = "Outlier Scores Density Estimate",
     xlab = "ALSO Score")
```

### Explore function output

```{r}
names(rf_also)

lapply(rf_also, class)
```

### Outlier scores

```{r}
plot(density(rf_also$scores), main = "Outlier Scores Density Estimate",
     xlab = "ALSO Score")
```

Present of outliers reflected in the severe right skewness of the distribution of outlier scores.

### Feature weights

```{r}
range(rf_also$adjusted_feature_weights)

rf_also$adjusted_feature_weights %>% 
    sort(decreasing = T) %>%
    barplot(las = 2, main = "Adjusted feature weights")
```

The ALSO technique using a random forest as the regressor is detecting outliers primarily on the basis of Murder, HSGrad, Illiteracy, and LifeExp, though it appears that the features are only moderately correlated.

# Dimension reduction via PCA with visualization of scores

Principal components analysis (PCA) is a great technique for visualizing multidimensional data in fewer dimensions. 

```{r}
pca_states <- princomp(states_scaled)

pca_states %>%
    QuickR::print_pca(.)
```

It appears that the top 3 principal components explain nearly 80% of the variation in the original dataset. Let's inspect a 3-D scatterplot for potential outliers.

```{r}
pca_states$scores %>%
    as_tibble() %>%
    mutate(outlier_scores = rf_also$scores) %>%
    plot_ly(x = ~Comp.1, y = ~Comp.2, z = ~Comp.3, type = "scatter3d")
```
