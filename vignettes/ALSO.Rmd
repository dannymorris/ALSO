---
title: "Attribute Wise Learning for Scoring Outliers"
author: "Danny Morris"
date: "July 18, 2018"
output: 
    html_document:
        theme: lumen
        toc: true
        df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      cache = FALSE)
```

# Overview

Attirubte-wise Learning for SCoring Outliers (ALSO) is an outlier detection technique for standard multidimensional datasets. For each variable in the dataset, a predictive model is constructed in which the remaining features are predictors. When a feature is generally predictable with good accuracy, observations deviating significantly from predicted values may be outliers. Observations with significant deviations across many predictable features may be strong outliers.

This document will illustrate the use of the **ALSO package**. It was created for the sole purpose of making the ALSO technique easily available to R users. The package currently contains a single function, `ALSO()`.

```{r}
# devtools::install_github("dannymorris/ALSO")
library(ALSO)

library(dplyr)
```

# Data

```{r}
states <- state.x77 %>% as_tibble()

states
```

This dataset is relatively small at 50 rows and 8 variables. Later on we'll explore a higher dimenional dataset and observe the results.

# Data Processing

It is customary to standardize numeric variables prior to predictive modeling in order to eliminate the effects of differences in original measurement scales. Let's apply the common z-standardization technique.

```{r}
states_scaled <- states %>%
    mutate_all(funs(scale))
```

# ALSO with Random Forest

A random forest works quite nicely in the context of ALSO for a few reasons:

1. can do prediction or classification
2. robust due to ensembling
3. can detect nonlinear relationships

```{r}
rf_also <- ALSO(data = states_scaled, model_function = randomForest::randomForest,
                cross_validate = FALSE, scores_only = F)
```

### Output contents

```{r}
names(rf_also)
```

### Outlier scores

```{r}
plot(density(rf_also$scores), main = "Outlier Scores Density Estimate",
     xlab = "ALSO Score")
```

Present of outliers reflected in the severe right skewness of the distribution of outlier scores.

### Feature weights

```{r}
rf_also$adjusted_feature_weights %>% 
    sort(decreasing = T) %>%
    barplot(las = 2, main = "Adjusted Feature Weights")
```

The ALSO technique using a random forest as the regressor is detecting outliers on the basis of Murder, HSGrad, Illiteracy, and LifeExp. Note that none of the features are very predictable.

# Visualization

Principal components analysis is a great technique for visualizing multidimensional data in fewer dimensions.

```{r}
pca_states <- princomp(states_scaled)
summary(pca_states, loadings = T)
```

It appears that the top 3 principal components explain nearly 80% of the variation in the original dataset.

```{r}
pca_states$scores %>%
    as_tibble() %>%
    mutate(outlier_scores = rf_also$scores) %>%
    ggplot(aes(x = Comp.1, y = Comp.2)) +
    geom_point(aes(color = outlier_scores), size = 3) +
    theme_minimal()
```
